{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#General-utils\" data-toc-modified-id=\"General-utils-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>General utils</a></span></li><li><span><a href=\"#Torch-utils\" data-toc-modified-id=\"Torch-utils-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Torch utils</a></span></li><li><span><a href=\"#BERT-utils\" data-toc-modified-id=\"BERT-utils-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>BERT utils</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n",
    "> A set of utility functions used throughout the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def flatten_dict(nested, sep='/'):\n",
    "    \"\"\"Flatten dictionary and concatenate nested keys with separator.\"\"\"\n",
    "    def rec(nest, prefix, into):\n",
    "        for k, v in nest.items():\n",
    "            if sep in k:\n",
    "                raise ValueError(f\"separator '{sep}' not allowed to be in key '{k}'\")\n",
    "            if isinstance(v, collections.Mapping):\n",
    "                rec(v, prefix + k + sep, into)\n",
    "            else:\n",
    "                into[prefix + k] = v\n",
    "    flat = {}\n",
    "    rec(nested, '', flat)\n",
    "    return flat\n",
    "\n",
    "def stack_dicts(stats_dicts):\n",
    "    \"\"\"Stack the values of a dict.\"\"\"\n",
    "    results = dict()\n",
    "    for k in stats_dicts[0]:\n",
    "        stats_list = [torch.flatten(d[k]) for d in stats_dicts]\n",
    "        results[k] = torch.stack(stats_list)\n",
    "    return results\n",
    "\n",
    "def add_suffix(input_dict, suffix):\n",
    "    \"\"\"Add suffix to dict keys.\"\"\"\n",
    "    return dict((k + suffix, v) for k,v in input_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def pad_to_size(tensor, size, dim=1, padding=50256):\n",
    "    \"\"\"Pad tensor to size.\"\"\"\n",
    "    t_size = tensor.size()[dim]\n",
    "    if t_size==size:\n",
    "        return tensor\n",
    "    else:\n",
    "        return torch.nn.functional.pad(tensor, (0,size-t_size), 'constant', padding)\n",
    "\n",
    "def logprobs_from_logits(logits, labels):\n",
    "    \"\"\"\n",
    "    See: https://github.com/pytorch/pytorch/issues/563#issuecomment-330103591\n",
    "    \"\"\"\n",
    "    logp = F.log_softmax(logits, dim=2)\n",
    "    logpy = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logpy\n",
    "\n",
    "\n",
    "def whiten(values, shift_mean=True):\n",
    "    \"\"\"Whiten values.\"\"\"\n",
    "    mean, var = torch.mean(values), torch.var(values)\n",
    "    whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n",
    "    if not shift_mean:\n",
    "        whitened += mean\n",
    "    return whitened\n",
    "\n",
    "def clip_by_value(x, tensor_min, tensor_max):\n",
    "    \"\"\"\n",
    "    Tensor extenstion to torch.clamp\n",
    "    https://github.com/pytorch/pytorch/issues/2793#issuecomment-428784713\n",
    "    \"\"\"\n",
    "    clipped = torch.max(torch.min(x, tensor_max), tensor_min)\n",
    "    return clipped\n",
    "\n",
    "def entropy_from_logits(logits):\n",
    "    \"\"\"Calculate entropy from logits.\"\"\"\n",
    "    pd = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    entropy = torch.logsumexp(logits, axis=-1) - torch.sum(pd*logits, axis=-1)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def average_torch_dicts(list_of_dicts):\n",
    "    \"\"\"Average values of a list of dicts wiht torch tensors.\"\"\"\n",
    "    average_dict = dict()\n",
    "    for key in list_of_dicts[0].keys():\n",
    "        average_dict[key] = torch.mean(torch.stack([d[key] for d in list_of_dicts]), axis=0)\n",
    "    return average_dict\n",
    "\n",
    "def stats_to_np(stats_dict):\n",
    "    \"\"\"Cast all torch.tensors in dict to numpy arrays.\"\"\"\n",
    "    new_dict = dict()\n",
    "    for k, v in stats_dict.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            new_dict[k] = v.detach().cpu().numpy()\n",
    "        else:\n",
    "            new_dict[k] = v\n",
    "        if np.isscalar(new_dict[k]):\n",
    "            new_dict[k] = float(new_dict[k])\n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def build_bert_batch_from_txt(text_list, tokenizer, device):\n",
    "    \"\"\"Create token id and attention mask tensors from text list for BERT classification.\"\"\"\n",
    "    \n",
    "    # tokenize\n",
    "    tensors = [tokenizer.encode(txt, return_tensors=\"pt\").to(device) for txt in text_list]\n",
    "    \n",
    "    # find max length to pad to\n",
    "    max_len = max([t.size()[1] for t in tensors])\n",
    "    \n",
    "    # get padded tensors and attention masks\n",
    "    # (attention masks make bert ignore padding)\n",
    "    padded_tensors = []\n",
    "    attention_masks = []\n",
    "    for tensor in tensors:\n",
    "        attention_mask = torch.ones(tensor.size(), device=device)\n",
    "        padded_tensors.append(pad_to_size(tensor, max_len, padding=0))\n",
    "        attention_masks.append(pad_to_size(attention_mask, max_len, padding=0))\n",
    "    \n",
    "    # stack all tensors\n",
    "    padded_tensors = torch.cat(padded_tensors)\n",
    "    attention_masks = torch.cat(attention_masks)  \n",
    "    \n",
    "    return padded_tensors, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Why-a-value-head?\" data-toc-modified-id=\"Why-a-value-head?-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Why a value head?</a></span></li><li><span><a href=\"#Detach-head\" data-toc-modified-id=\"Detach-head-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Detach head</a></span></li><li><span><a href=\"#Load-a-pre-trained-language-model\" data-toc-modified-id=\"Load-a-pre-trained-language-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load a pre-trained language model</a></span></li><li><span><a href=\"#Forward-pass\" data-toc-modified-id=\"Forward-pass-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Forward pass</a></span></li><li><span><a href=\"#Model-outputs\" data-toc-modified-id=\"Model-outputs-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model outputs</a></span></li><li><span><a href=\"#Batched-response-to-queries\" data-toc-modified-id=\"Batched-response-to-queries-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Batched response to queries</a></span></li><li><span><a href=\"#Why-the-custom-response-function?\" data-toc-modified-id=\"Why-the-custom-response-function?-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Why the custom response function?</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-model-reward\" data-toc-modified-id=\"The-model-reward-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>The model reward</a></span></li><li><span><a href=\"#Case-1:-min_length=None\" data-toc-modified-id=\"Case-1:-min_length=None-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Case 1: <code>min_length=None</code></a></span></li><li><span><a href=\"#Case-2:-min_length=max_length\" data-toc-modified-id=\"Case-2:-min_length=max_length-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Case 2: <code>min_length=max_length</code></a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 with value head\n",
    "> A GPT2 model with a value head built on the `transformer` library by Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why a value head?\n",
    "Optimisation through PPO requires estimates on the current states value. The value can be estimated by adding a second head to the GPT2 model which outputs a scalar for each output token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detach head\n",
    "I experimented with detaching the head from the body when optimizing the model. This means that only the head is trained and the gradients are not passed through the body. Although I did not use it in the end it is still possible to detach the head by calling `model.detach_head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package             Version             Location\n",
      "------------------- ------------------- -----------------------------------\n",
      "appnope             0.1.2\n",
      "argon2-cffi         20.1.0\n",
      "async-generator     1.10\n",
      "attrs               20.3.0\n",
      "backcall            0.2.0\n",
      "bleach              3.3.0\n",
      "boto3               1.17.16\n",
      "botocore            1.20.16\n",
      "certifi             2020.12.5\n",
      "cffi                1.14.5\n",
      "chardet             4.0.0\n",
      "click               7.1.2\n",
      "configparser        5.0.1\n",
      "cycler              0.10.0\n",
      "dataclasses         0.8\n",
      "decorator           4.4.2\n",
      "defusedxml          0.6.0\n",
      "docker-pycreds      0.4.0\n",
      "entrypoints         0.3\n",
      "fastscript          1.0.0\n",
      "filelock            3.0.12\n",
      "gitdb               4.0.5\n",
      "GitPython           3.1.13\n",
      "gql                 0.2.0\n",
      "graphql-core        1.1\n",
      "idna                2.10\n",
      "importlib-metadata  3.7.0\n",
      "ipykernel           5.5.0\n",
      "ipython             7.16.1\n",
      "ipython-genutils    0.2.0\n",
      "ipywidgets          7.6.3\n",
      "jedi                0.18.0\n",
      "Jinja2              2.11.3\n",
      "jmespath            0.10.0\n",
      "joblib              1.0.1\n",
      "json5               0.9.5\n",
      "jsonschema          3.2.0\n",
      "jupyter-client      6.1.11\n",
      "jupyter-core        4.7.1\n",
      "jupyterlab          2.0.1\n",
      "jupyterlab-pygments 0.1.2\n",
      "jupyterlab-server   1.0.9\n",
      "jupyterlab-widgets  1.0.0\n",
      "kiwisolver          1.3.1\n",
      "MarkupSafe          1.1.1\n",
      "matplotlib          3.2.1\n",
      "mistune             0.8.4\n",
      "nbclient            0.5.2\n",
      "nbconvert           6.0.7\n",
      "nbdev               0.2.16\n",
      "nbformat            5.1.2\n",
      "nest-asyncio        1.5.1\n",
      "notebook            6.2.0\n",
      "numpy               1.18.2\n",
      "nvidia-ml-py3       7.352.0\n",
      "packaging           20.9\n",
      "pandas              1.0.3\n",
      "pandocfilters       1.4.3\n",
      "parso               0.8.1\n",
      "pexpect             4.8.0\n",
      "pickleshare         0.7.5\n",
      "Pillow              8.1.0\n",
      "pip                 21.0.1\n",
      "prometheus-client   0.9.0\n",
      "promise             2.3\n",
      "prompt-toolkit      3.0.16\n",
      "protobuf            3.15.3\n",
      "psutil              5.8.0\n",
      "ptyprocess          0.7.0\n",
      "pycparser           2.20\n",
      "Pygments            2.8.0\n",
      "pyparsing           2.4.7\n",
      "pyrsistent          0.17.3\n",
      "python-dateutil     2.8.1\n",
      "pytz                2021.1\n",
      "PyYAML              5.4.1\n",
      "pyzmq               22.0.3\n",
      "regex               2020.11.13\n",
      "requests            2.25.1\n",
      "s3transfer          0.3.4\n",
      "sacremoses          0.0.43\n",
      "scikit-learn        0.24.1\n",
      "scipy               1.5.4\n",
      "Send2Trash          1.5.0\n",
      "sentencepiece       0.1.95\n",
      "sentry-sdk          0.20.3\n",
      "seqeval             1.2.2\n",
      "setuptools          52.0.0.post20210125\n",
      "shortuuid           1.0.1\n",
      "simpletransformers  0.21.4\n",
      "six                 1.15.0\n",
      "smmap               3.0.5\n",
      "subprocess32        3.5.4\n",
      "tensorboardX        2.1\n",
      "terminado           0.9.2\n",
      "testpath            0.4.4\n",
      "threadpoolctl       2.1.0\n",
      "tokenizers          0.5.2\n",
      "torch               1.7.1\n",
      "torchvision         0.8.2\n",
      "tornado             6.1\n",
      "tqdm                4.43.0\n",
      "traitlets           4.3.3\n",
      "transformers        2.6.0\n",
      "trl                 0.0.1               /Users/joshdey/Desktop/FatBrain/trl\n",
      "typing-extensions   3.7.4.3\n",
      "urllib3             1.26.3\n",
      "wandb               0.8.35\n",
      "watchdog            2.0.2\n",
      "wcwidth             0.2.5\n",
      "webencodings        0.5.1\n",
      "wheel               0.36.2\n",
      "widgetsnbextension  3.5.1\n",
      "zipp                3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model, GPT2PreTrainedModel\n",
    "from transformers.modeling_utils import top_k_top_p_filtering\n",
    "from torch import nn\n",
    "from torch.nn import Identity\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "class ValueHead(nn.Module):\n",
    "    \"\"\"The ValueHead class implements a head for GPT2 that returns a scalar for each output token.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.detach_head = False\n",
    "        self.summary_type = config.summary_type if hasattr(config, \"summary_type\") else \"last\"\n",
    "        if self.summary_type == \"attn\":\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.summary = Identity()\n",
    "        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n",
    "            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n",
    "                num_classes = config.num_labels\n",
    "            else:\n",
    "                num_classes = config.hidden_size\n",
    "            self.summary = nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "        self.activation = Identity()\n",
    "        if hasattr(config, \"summary_activation\") and config.summary_activation == \"tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        self.first_dropout = Identity()\n",
    "        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n",
    "            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n",
    "\n",
    "        self.last_dropout = Identity()\n",
    "        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n",
    "            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n",
    "            \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, hidden_states, cls_index=None):\n",
    "        if self.detach_head:\n",
    "            output = hidden_states.detach()\n",
    "        else:\n",
    "            output = hidden_states\n",
    "        output = self.first_dropout(output)\n",
    "        output = self.summary(output)\n",
    "        output = self.activation(output)\n",
    "        output = self.last_dropout(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "class GPT2HeadWithValueModel(GPT2PreTrainedModel):\n",
    "    \"\"\"The GPT2HeadWithValueModel class implements a GPT2 language model with a secondary, scalar head.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        config.num_labels = 1\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.v_head = ValueHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def detach_value_head(self):\n",
    "        self.v_head.detach_head = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        mc_token_ids=None,\n",
    "        lm_labels=None,\n",
    "        mc_labels=None,\n",
    "    ):\n",
    "       \n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past=past,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        value = self.v_head(hidden_states).squeeze(-1)\n",
    "\n",
    "        outputs = (lm_logits,) + transformer_outputs[1:] + (value,)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pre-trained language model\n",
    "Loading a pretrained language model works like loading it with a model from the `transformer` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2HeadWithValueModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"I liked the movie Transformers!\" + tokenizer.eos_token\n",
    "input_ids = tokenizer.encode(input_txt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "logits, transformer_outputs, values = model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We input a batch of `1` with `7` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logits tensor is of shape `[batch_size, num_input_tokens, vocab_size]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 50257])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value tensor is of shape `[batch_size, num_input_tokens]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can greedy decode the next token predictions from the logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ids = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> .\n",
      " liked -->  the\n",
      " the -->  idea\n",
      " movie --> ,\n",
      " Transformers --> ,\n",
      "! -->  I\n",
      "<|endoftext|> --> The\n"
     ]
    }
   ],
   "source": [
    "for i in range(input_ids.shape[1]):\n",
    "    current_id = tokenizer.decode(input_ids[:, i])\n",
    "    next_id = tokenizer.decode(pred_ids[:, i])\n",
    "    print(current_id, '-->', next_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched response to queries\n",
    "To speed up computations it helps to process queries in a batched fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def respond_to_batch(model, queries, txt_len=20, top_k=0, top_p=1.0):\n",
    "    \"\"\"Sample text from language model.\"\"\"\n",
    "    input_ids = queries\n",
    "    for i in range(txt_len):\n",
    "        # Get Logits\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs[0][:, -1, :]\n",
    "        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "        # Sample\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "    return input_ids[:, -txt_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the model respond to two queries in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 5]), torch.Size([1, 5])]\n"
     ]
    }
   ],
   "source": [
    "query_txt_1 = \"My most favourite movie is\"\n",
    "query_txt_2 = \"My least favourite movie is\"\n",
    "queries_txt = [query_txt_1, query_txt_2]\n",
    "\n",
    "queries = [tokenizer.encode(query_txt, return_tensors=\"pt\") for query_txt in queries_txt]\n",
    "print([q.shape for q in queries])\n",
    "queries = torch.cat(queries)\n",
    "\n",
    "responses = respond_to_batch(model, queries, txt_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This only works because both queries have the same number of tokens. If that is not the case one must pad the tensors before stacking them in `torch.cat(queries)`.\n",
    "\n",
    "Then we can decode the responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My most favourite movie is King's Quest. My favourite movie is Blade Runner\n",
      "My least favourite movie is the one with Cinderella. I tend to follow those\n"
     ]
    }
   ],
   "source": [
    "for i in range(responses.shape[0]):\n",
    "    response_txt = tokenizer.decode(responses[i])\n",
    "    query_txt = queries_txt[i]\n",
    "    print(query_txt + response_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the custom response function?\n",
    "The models in the `transformer` library come with a very useful and optimised generation function `model.generate()`. In the beginning this function was indeed used to generate text but after lengthy debugging it turned out that PPO was exploiting some aspects that are generally useful for text generation but allowed the model to abuse it and gain extra rewards.\n",
    "\n",
    "### The model reward\n",
    "To understand how the model was able to exploit the generation function it is worth looking at the reward function for language modeling with PPO. The reward consists of an arbitrary score (any scalar to indicate whether the model output was good or bad) and the KL-divergence from the untrained model:\n",
    "\n",
    "$$reward = score - \\beta \\times KL$$\n",
    "\n",
    "where $\\beta$ is some positive factor. The KL divergence is calculate with:\n",
    "\n",
    "$$ KL = \\mathbb{E}_{x \\sim p_{model}} [\\log p_{model}(x) - \\log p_{refmodel}(x)]$$\n",
    "\n",
    "Since $x$ is sampled from $p_{model}$ the KL-divergence is always positive. However, if the model found a way to get negative KL-divergence it would achieve a positive reward. This is what happened twice with in the experiment and both times a quirk of the text generation was abused to avoid proper sampling from the probability distribution.\n",
    "\n",
    "### Case 1: `min_length=None`\n",
    "When no `min_length` is specified in the `model.generate()` function the model probability distribution is normally sampled until the first `<eos>` token appears. Then the rest of the sequence is padded with a padding token until `max_length` is reached (for GPT2 this is also the `<eos>` token). If that sequence is again passed through the model to evaluate the log-probabilities everything is normal until after the first `<eos>` token, since multiple `<eos>` tokens are very unlikely. The model exploited this by decreasing the probability for the `<eos>` token after the first appearence even further below the probability of the reference model, thus achieving negative KL-divergence. Additionally, it inserted the first `<eos>` earlier and earlier in the sentences to minimize the KL-divergence and thus maximise the reward. This only worked because the sequence after the first `<eos>` token wasn't properly sampled but padded, otherwise the low probabilities would have lead to other tokens with higher probability being sampled.\n",
    "\n",
    "\n",
    "### Case 2: `min_length=max_length`\n",
    "I thought this could be easily fixed: just set the `min_length=max_length`. This seemed to work fine for a few experiments until the training failed again due to negative KL-divergences. Finding the problem was harder than before, since it only happened rarely after several training steps. In addition the generated sentences deteriorated quickly to complete gibberish. After some investigation it turned out that the model was again exploiting the sampling function. Up to this point I was not aware that the model was also not allowed to produce an `<eos>` token before `min_length` is reached. In practice this is achieved by setting the next token logit to -infinity:\n",
    "\n",
    "```\n",
    "next_token_logits[:, eos_token_id] = -float(\"inf\")\n",
    "```\n",
    "\n",
    "This makes sure that after the softmax function the probability for the `<eos>` token is zero, no matter the model output. The model exploited this by maximizing the logit output for that token and thus setting all other logits to increasingly small numbers. Since, I did not apply the same step when evaluating the generated sequence (calculating softmax without the -inf trick) the probabilities for the generated sequences were extremely small and in fact smaller than the probabilities of the reference model. This lead again to negative KL-divergence.\n",
    "\n",
    "### Conclusion\n",
    "In both cases $x \\sim p_{model}$ in the KL-divergence equation was not satisfied, but this was hidden in the sequence generating function. Reinforcement Learning is very effective in finding and exploiting environment quirks as others have pointed out for other environments such as ATARI games. The solution was to go back to a simpler sequence sampler to avoid this exploits. Alternatively, I could have applied the same tricks and some masking to the model outputs when evaluating the sequences, but I didn't feel confident enough that there would not be other sequence generation tricks the model could abuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
